{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from llm import LLM, BaseLLM\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class ExecutionStep:\n",
    "    \"\"\"Enhanced execution step data class\"\"\"\n",
    "    step_id: int\n",
    "    description: str\n",
    "    prompt: str\n",
    "    expected_output: Optional[str] = None\n",
    "    key_points: Optional[List[str]] = None\n",
    "    constraints: Optional[List[str]] = None\n",
    "    dependencies: Optional[List[str]] = None\n",
    "    result: Optional[str] = None\n",
    "    check_result: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class TaskDecomposition:\n",
    "    def __init__(self, llm: LLM):\n",
    "        self.llm: BaseLLM = llm\n",
    "        self.conversation_history: List[Dict[str, str]] = []\n",
    "        \n",
    "    def _add_to_history(self, role: str, content: str):\n",
    "        \"\"\"Add conversation to history\"\"\"\n",
    "        self.conversation_history.append({\"role\": role, \"content\": content})\n",
    "        \n",
    "    def generate_plan(self, task_description: str) -> List[ExecutionStep]:\n",
    "        \"\"\"Generate detailed task execution plan\"\"\"\n",
    "        planning_prompt = f\"\"\"\n",
    "        Please analyze the following task and create a detailed, step-by-step execution plan.\n",
    "\n",
    "        Task Description: {task_description}\n",
    "\n",
    "        Generate a comprehensive execution plan in JSON format following these guidelines:\n",
    "\n",
    "        1. Step Structure:\n",
    "        {{\n",
    "            \"steps\": [\n",
    "                {{\n",
    "                    \"step_id\": 1,\n",
    "                    \"description\": \"Detailed step description\",\n",
    "                    \"prompt\": \"Specific execution instructions\",\n",
    "                    \"expected_output\": \"Description of what this step should produce\",\n",
    "                    \"key_points\": [\"Key elements to address\", \"Important aspects to include\"],\n",
    "                    \"constraints\": [\"Any limitations or requirements to consider\"],\n",
    "                    \"dependencies\": [\"References to previous steps if any\"]\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "\n",
    "        2. Requirements for Each Step:\n",
    "        - Description should be specific and actionable\n",
    "        - Prompt should provide clear guidance and context\n",
    "        - Include all necessary details for execution\n",
    "        - Consider dependencies on previous steps\n",
    "        - Specify quality criteria and expectations\n",
    "\n",
    "        3. Step Planning Considerations:\n",
    "        - Break down complex tasks into manageable pieces\n",
    "        - Ensure logical progression between steps\n",
    "        - Include specific details and examples where relevant\n",
    "        - Consider edge cases and potential challenges\n",
    "        - Maintain focus on overall task objectives\n",
    "\n",
    "        4. Content Guidelines:\n",
    "        - Be specific rather than generic\n",
    "        - Include measurable outcomes\n",
    "        - Provide context for each step\n",
    "        - Specify any required research or reference materials\n",
    "        - Include quality checks and validation criteria\n",
    "\n",
    "        5. Example of Detailed Step:\n",
    "        {{\n",
    "            \"step_id\": 1,\n",
    "            \"description\": \"Research and analyze the historical context of Victorian England\",\n",
    "            \"prompt\": \"Research and document key aspects of Victorian England (1837-1901): social classes, daily life, technological advancements, and cultural norms. Focus on aspects that directly influence the story's setting. Include specific examples of: clothing styles, transportation methods, social etiquette, and common occupations. Consider how these elements will affect character behavior and plot development.\",\n",
    "            \"expected_output\": \"A comprehensive summary of relevant Victorian era details\",\n",
    "            \"key_points\": [\"Social hierarchy\", \"Technology level\", \"Cultural norms\", \"Daily life aspects\"],\n",
    "            \"constraints\": [\"Must be historically accurate\", \"Focus on story-relevant details\"],\n",
    "            \"dependencies\": []\n",
    "        }}\n",
    "\n",
    "        Remember:\n",
    "        - Each step should be self-contained but connected to the overall goal\n",
    "        - Include enough detail for accurate execution\n",
    "        - Consider how each step builds towards the final outcome\n",
    "        - Provide clear success criteria for each step\n",
    "        - Include specific examples or references where helpful\n",
    "\n",
    "        Please ensure the generated plan is detailed enough that each step can be executed without requiring additional clarification.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._add_to_history(\"user\", planning_prompt)\n",
    "        \n",
    "        # Generate execution plan\n",
    "        plan_json = self.llm.generate_json(\n",
    "            prompt=planning_prompt,\n",
    "            schema={\n",
    "                \"steps\": list\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if not plan_json or \"steps\" not in plan_json:\n",
    "            raise ValueError(\"Failed to generate plan\")\n",
    "            \n",
    "        self._add_to_history(\"assistant\", json.dumps(plan_json, ensure_ascii=False, indent=2))\n",
    "        \n",
    "        # Convert to ExecutionStep objects\n",
    "        steps = []\n",
    "        for step in plan_json[\"steps\"]:\n",
    "            steps.append(ExecutionStep(\n",
    "                step_id=step[\"step_id\"],\n",
    "                description=step[\"description\"],\n",
    "                prompt=step[\"prompt\"]\n",
    "            ))\n",
    "            \n",
    "        return steps\n",
    "        \n",
    "    def execute_step(self, step: ExecutionStep, previous_results: List[str]) -> str:\n",
    "        \"\"\"Execute a single step with enhanced context and guidance\"\"\"\n",
    "        # Build comprehensive context for step execution\n",
    "        dependencies_context = self._build_dependencies_context(step, previous_results)\n",
    "        \n",
    "        context_prompt = f\"\"\"\n",
    "        Task Execution Step {step.step_id}\n",
    "\n",
    "        Step Description: {step.description}\n",
    "\n",
    "        Previous Context:\n",
    "        {dependencies_context}\n",
    "\n",
    "        Expected Output: {step.expected_output}\n",
    "\n",
    "        Key Points to Address:\n",
    "        {self._format_list(step.key_points)}\n",
    "\n",
    "        Constraints to Consider:\n",
    "        {self._format_list(step.constraints)}\n",
    "\n",
    "        Detailed Instructions:\n",
    "        {step.prompt}\n",
    "\n",
    "        Requirements for Execution:\n",
    "        1. Address all key points explicitly\n",
    "        2. Follow all specified constraints\n",
    "        3. Maintain alignment with previous steps\n",
    "        4. Ensure output format matches expectations\n",
    "        5. Focus on quality and completeness\n",
    "\n",
    "        Please execute this step and provide a detailed response that meets all requirements.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._add_to_history(\"user\", context_prompt)\n",
    "        \n",
    "        result = self.llm.generate_text(\n",
    "            prompt=context_prompt,\n",
    "            max_tokens=2000\n",
    "        )\n",
    "        \n",
    "        self._add_to_history(\"assistant\", result)\n",
    "        return result\n",
    "\n",
    "    def check_step_result(self, step: ExecutionStep, result: str, task_description: str) -> Dict[str, Any]:\n",
    "        \"\"\"Enhanced result validation with detailed feedback\"\"\"\n",
    "        check_prompt = f\"\"\"\n",
    "        Please perform a comprehensive evaluation of the step execution result.\n",
    "\n",
    "        Original Task: {task_description}\n",
    "\n",
    "        Step Information:\n",
    "        - Description: {step.description}\n",
    "        - Expected Output: {step.expected_output}\n",
    "        - Key Points: {self._format_list(step.key_points)}\n",
    "        - Constraints: {self._format_list(step.constraints)}\n",
    "\n",
    "        Execution Result:\n",
    "        {result}\n",
    "\n",
    "        Please evaluate the result based on the following criteria and return a detailed analysis in JSON format:\n",
    "\n",
    "        {{\n",
    "            \"passed\": boolean,\n",
    "            \"scores\": {{\n",
    "                \"completeness\": (0-10),  // Did it address all required points?\n",
    "                \"constraints_met\": (0-10),  // Were all constraints followed?\n",
    "                \"quality\": (0-10),  // Overall quality of the output\n",
    "                \"coherence\": (0-10)  // Logical flow and connection with other steps\n",
    "            }},\n",
    "            \"analysis\": {{\n",
    "                \"strengths\": [\"list\", \"of\", \"strengths\"],\n",
    "                \"weaknesses\": [\"list\", \"of\", \"weaknesses\"],\n",
    "                \"missing_points\": [\"key points\", \"not addressed\"],\n",
    "                \"violated_constraints\": [\"constraints\", \"not met\"]\n",
    "            }},\n",
    "            \"improvement_suggestions\": [\"specific\", \"actionable\", \"suggestions\"],\n",
    "            \"overall_feedback\": \"Detailed explanation of the evaluation\"\n",
    "        }}\n",
    "\n",
    "        Provide specific examples and references when discussing strengths or weaknesses.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._add_to_history(\"user\", check_prompt)\n",
    "        \n",
    "        check_result = self.llm.generate_json(\n",
    "            prompt=check_prompt,\n",
    "            schema={\n",
    "                \"passed\": bool,\n",
    "                \"scores\": dict,\n",
    "                \"analysis\": dict,\n",
    "                \"improvement_suggestions\": list,\n",
    "                \"overall_feedback\": str\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self._add_to_history(\"assistant\", json.dumps(check_result, ensure_ascii=False, indent=2))\n",
    "        return check_result\n",
    "\n",
    "    def _build_dependencies_context(self, step: ExecutionStep, previous_results: List[str]) -> str:\n",
    "        \"\"\"Build context based on step dependencies\"\"\"\n",
    "        if not step.dependencies or not previous_results:\n",
    "            return \"No dependencies on previous steps.\"\n",
    "            \n",
    "        context = \"Relevant context from previous steps:\\n\\n\"\n",
    "        for dep in step.dependencies:\n",
    "            step_num = int(dep.split()[-1]) - 1  # Extract step number from dependency\n",
    "            if 0 <= step_num < len(previous_results):\n",
    "                context += f\"From {dep}:\\n{previous_results[step_num]}\\n\\n\"\n",
    "        return context\n",
    "\n",
    "    def _format_list(self, items: Optional[List[str]]) -> str:\n",
    "        \"\"\"Format list items for prompt display\"\"\"\n",
    "        if not items:\n",
    "            return \"None specified\"\n",
    "        return \"\\n\".join(f\"- {item}\" for item in items)\n",
    "\n",
    "    def execute_task(self, task_description: str, enable_checking: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Execute complete task with enhanced monitoring and control\"\"\"\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        # Generate initial plan\n",
    "        steps = self.generate_plan(task_description)\n",
    "        \n",
    "        # Execute steps sequentially\n",
    "        results = []\n",
    "        execution_log = []\n",
    "        \n",
    "        for step in steps:\n",
    "            # Execute step\n",
    "            result = self.execute_step(step, results)\n",
    "            step.result = result\n",
    "            \n",
    "            # Validate result if enabled\n",
    "            if enable_checking:\n",
    "                check_result = self.check_step_result(step, result, task_description)\n",
    "                step.check_result = check_result\n",
    "                \n",
    "                # Retry if necessary\n",
    "                if not check_result[\"passed\"]:\n",
    "                    retry_prompt = self._generate_retry_prompt(step, check_result)\n",
    "                    result = self.llm.generate_text(\n",
    "                        prompt=retry_prompt,\n",
    "                        max_tokens=2000\n",
    "                    )\n",
    "                    step.result = result\n",
    "                    \n",
    "                    # Re-check after retry\n",
    "                    step.check_result = self.check_step_result(step, result, task_description)\n",
    "            \n",
    "            results.append(result)\n",
    "            execution_log.append({\n",
    "                \"step_id\": step.step_id,\n",
    "                \"description\": step.description,\n",
    "                \"expected_output\": step.expected_output,\n",
    "                \"result\": step.result,\n",
    "                \"check_result\": step.check_result\n",
    "            })\n",
    "        \n",
    "        # Generate final result with enhanced context\n",
    "        final_result = self._generate_final_result(task_description, results, steps)\n",
    "        \n",
    "        return {\n",
    "            \"final_result\": final_result,\n",
    "            \"execution_log\": execution_log,\n",
    "            \"conversation_history\": self.conversation_history\n",
    "        }\n",
    "\n",
    "    def _generate_retry_prompt(self, step: ExecutionStep, check_result: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate detailed retry prompt based on validation feedback\"\"\"\n",
    "        return f\"\"\"\n",
    "        The previous execution of step {step.step_id} requires improvement.\n",
    "\n",
    "        Original Description: {step.description}\n",
    "        Expected Output: {step.expected_output}\n",
    "\n",
    "        Previous Result: {step.result}\n",
    "\n",
    "        Evaluation Feedback:\n",
    "        - Scores: {json.dumps(check_result['scores'], indent=2)}\n",
    "        - Missing Points: {', '.join(check_result['analysis']['missing_points'])}\n",
    "        - Violated Constraints: {', '.join(check_result['analysis']['violated_constraints'])}\n",
    "\n",
    "        Improvement Requirements:\n",
    "        {self._format_list(check_result['improvement_suggestions'])}\n",
    "\n",
    "        Please revise the output addressing all identified issues while maintaining:\n",
    "        1. Original task objectives\n",
    "        2. Consistency with previous steps\n",
    "        3. All specified constraints\n",
    "        4. Required quality standards\n",
    "\n",
    "        Provide an improved version that addresses all feedback points.\n",
    "        \"\"\"\n",
    "\n",
    "    def _generate_final_result(self, task_description: str, results: List[str], steps: List[ExecutionStep]) -> str:\n",
    "        \"\"\"Generate final result with comprehensive context integration\"\"\"\n",
    "        final_integration_prompt = f\"\"\"\n",
    "        Please create a comprehensive final result integrating all completed steps.\n",
    "\n",
    "        Original Task Description:\n",
    "        {task_description}\n",
    "\n",
    "        Step Results Summary:\n",
    "        {self._format_steps_summary(steps, results)}\n",
    "\n",
    "        Requirements for Final Integration:\n",
    "        1. Ensure perfect alignment with original task requirements\n",
    "        2. Maintain logical flow and coherence across all components\n",
    "        3. Address all key points from individual steps\n",
    "        4. Resolve any inconsistencies between steps\n",
    "        5. Provide a polished and professional final output\n",
    "\n",
    "        Create a cohesive final result that successfully achieves all original task objectives while maintaining the quality and detail level of individual steps.\n",
    "        \"\"\"\n",
    "\n",
    "        self._add_to_history(\"user\", final_integration_prompt)\n",
    "        final_result = self.llm.generate_text(\n",
    "            prompt=final_integration_prompt,\n",
    "            max_tokens=3000\n",
    "        )\n",
    "        self._add_to_history(\"assistant\", final_result)\n",
    "        return final_result\n",
    "\n",
    "    def _format_steps_summary(self, steps: List[ExecutionStep], results: List[str]) -> str:\n",
    "        \"\"\"Format detailed summary of all steps and their results\"\"\"\n",
    "        summary = \"\"\n",
    "        for step, result in zip(steps, results):\n",
    "            summary += f\"\\nStep {step.step_id}: {step.description}\\n\"\n",
    "            summary += f\"Expected Output: {step.expected_output}\\n\"\n",
    "            summary += f\"Result: {result}\\n\"\n",
    "            if step.check_result:\n",
    "                summary += f\"Quality Scores: {json.dumps(step.check_result.get('scores', {}), indent=2)}\\n\"\n",
    "            summary += \"-\" * 80 + \"\\n\"\n",
    "        return summary\n",
    "\n",
    "# Usage example\n",
    "def example_usage(task:str = None):\n",
    "    llm = LLM(model_type='claude', verbose=True)\n",
    "    decomposer = TaskDecomposition(llm)\n",
    "\n",
    "    # Define task\n",
    "    task = \"\"\"\n",
    "Write a 500-word science fiction story about a time traveler who discovers \n",
    "an unexpected consequence of changing the past. Include character development, \n",
    "plot twists, and a meaningful resolution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute task with enhanced monitoring\n",
    "    result = decomposer.execute_task(task, enable_checking=True)\n",
    "\n",
    "    # Access detailed execution log\n",
    "    for entry in result[\"execution_log\"]:\n",
    "        print(f\"\\nStep {entry['step_id']}:\")\n",
    "        print(f\"Description: {entry['description']}\")\n",
    "        print(f\"Expected Output: {entry['expected_output']}\")\n",
    "        if entry['check_result']:\n",
    "            print(f\"Quality Scores: {entry['check_result']['scores']}\")\n",
    "            print(f\"Improvement Suggestions: {entry['check_result']['improvement_suggestions']}\")\n",
    "\n",
    "    # Print final result\n",
    "    print(\"\\nFinal Result:\")\n",
    "    print(result[\"final_result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type type is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mexample_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m(使用中文生成和思考)生成一个勇者战胜恶龙的故事，但是主角的同伴背叛他，并且主角有着悲惨的过去。最后主角付出了生命才战胜恶龙，但是成果被恶毒的伙伴侵吞，无人知晓主角的功绩。\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 370\u001b[0m, in \u001b[0;36mexample_usage\u001b[1;34m(task)\u001b[0m\n\u001b[0;32m    363\u001b[0m     task \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124mWrite a 500-word science fiction story about a time traveler who discovers \u001b[39m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;124man unexpected consequence of changing the past. Include character development, \u001b[39m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124mplot twists, and a meaningful resolution.\u001b[39m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;66;03m# Execute task with enhanced monitoring\u001b[39;00m\n\u001b[1;32m--> 370\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdecomposer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_checking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;66;03m# Access detailed execution log\u001b[39;00m\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution_log\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "Cell \u001b[1;32mIn[6], line 243\u001b[0m, in \u001b[0;36mTaskDecomposition.execute_task\u001b[1;34m(self, task_description, enable_checking)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconversation_history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# Generate initial plan\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_plan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_description\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# Execute steps sequentially\u001b[39;00m\n\u001b[0;32m    246\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[6], line 98\u001b[0m, in \u001b[0;36mTaskDecomposition.generate_plan\u001b[1;34m(self, task_description)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_to_history(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, planning_prompt)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Generate execution plan\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m plan_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplanning_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m plan_json \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m plan_json:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to generate plan\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\work\\LLM_6907_Project\\llm.py:123\u001b[0m, in \u001b[0;36mClaudeLLM.generate_json\u001b[1;34m(self, prompt, schema, max_tokens, temperature)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt, schema, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1500\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m# 构建更严格的JSON提示\u001b[39;00m\n\u001b[0;32m    120\u001b[0m     json_prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease complete the following JSON object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour response must start with the opening brace and end with the closing brace.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 123\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe example of schema is:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplete this JSON based on the following request: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    125\u001b[0m     )\n\u001b[0;32m    127\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    128\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m    129\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m         ]\n\u001b[0;32m    136\u001b[0m     )\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# 获取响应文本并确保它是完整的JSON\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Something_else\\python310\\lib\\json\\__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m--> 238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Something_else\\python310\\lib\\json\\encoder.py:201\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterencode(o, _one_shot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chunks)\n",
      "File \u001b[1;32mE:\\Something_else\\python310\\lib\\json\\encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mE:\\Something_else\\python310\\lib\\json\\encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mE:\\Something_else\\python310\\lib\\json\\encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    437\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[1;32m--> 438\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mE:\\Something_else\\python310\\lib\\json\\encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type type is not JSON serializable"
     ]
    }
   ],
   "source": [
    "example_usage('(使用中文生成和思考)生成一个勇者战胜恶龙的故事，但是主角的同伴背叛他，并且主角有着悲惨的过去。最后主角付出了生命才战胜恶龙，但是成果被恶毒的伙伴侵吞，无人知晓主角的功绩。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
